{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73aebb3-453d-4089-9299-2df8a8a4e263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating Schema Layers for Data Warehousing\n",
    "\n",
    "First, let's create the layers for each of the stages. Don't be confused by names, it's similar to Medallion architecture.\n",
    "\n",
    "The stage layer we create are:\n",
    "* Landing - Where raw data is put, but still partitioned/optimized for querying for downstream layers\n",
    "* Conformed - Where the cleaned and de-depulicated data is stored. Also, it will house only a subset of data that needs frequent querying\n",
    "* Consumption - Where the views and other reporting tables are created if needed (materialized) exposed to dashboarding tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd4fa76-a434-4fb7-ab23-0379fdd38855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "--Create Schema for Landing Layer\n",
    "CREATE SCHEMA IF NOT EXISTS LANDING;\n",
    "\n",
    "--Create Schema for Conformed Layer\n",
    "CREATE SCHEMA IF NOT EXISTS CONFORMED;\n",
    "\n",
    "--Create Schema for Consumption Layer\n",
    "CREATE SCHEMA IF NOT EXISTS CONSUMPTION;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf0c1ea-d254-42fe-bf2b-cb2a885f0f5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading, Transforming and Storing data\n",
    "\n",
    "In this section, the data is read from external locations to move in Databricks managed storage where it's transformed and moved to other layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f2fbadf-de0b-4454-b606-9b21247e493b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read data from external location\n",
    "\n",
    "Here we'll read the data from external location which we created previously. As you'll see, the number of partitions are same as number of CPU cores available. Also we are printing the size for each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b555f6ca-03f2-4059-9c63-6b06ce61d664",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of partitions :  12\n[INFO] Size Per Partition (MB) :  128.0\nroot\n |-- Year: integer (nullable = true)\n |-- Month: integer (nullable = true)\n |-- Day: integer (nullable = true)\n |-- UserID: string (nullable = true)\n |-- Clicks: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "\n",
    "file_path = 'gs://partition-store/output.csv'\n",
    "\n",
    "# Read the data and set the logical partitions through maxByteSize\n",
    "df = (spark.read\n",
    " .format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"delimiter\", \",\")\n",
    " .load(file_path)\n",
    " )\n",
    "\n",
    "print('[INFO] Number of partitions : ', df.rdd.getNumPartitions())\n",
    "print('[INFO] Size Per Partition (MB) : ', int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace('b',''))/(1024*1024) )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69223930-bd80-4a39-9786-9cb9b776a3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tranform Columns and Order by columns to optimize write operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e96c0e2-eebd-4fbf-a359-a2d4a157c87b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- userid: string (nullable = true)\n |-- clicks: integer (nullable = true)\n |-- date: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, concat, to_date, lit\n",
    "\n",
    "new_cols = {i:i.lower() for i in list(df.columns)}\n",
    "\n",
    "transformed_df = (\n",
    "    df\n",
    "    .withColumnsRenamed(new_cols)\n",
    "    .withColumn(\"date\", to_date(concat( col(\"year\"), lit('-'), col(\"month\"), lit('-'),col(\"day\")) ))\n",
    "    .drop(\"year\", \"month\", \"day\")\n",
    "    .orderBy(\"date\", \"userid\", \"clicks\")\n",
    "    )\n",
    "\n",
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06c7869-a2a1-4299-afb9-a4aa1e880a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Writing the transformed data to Databricks \n",
    "\n",
    "Data is moved to managed storage as Delta table. Plus it's partition by date since ETL job will operated on daily append to the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4fd0a6-f292-4546-9e02-cee849024ddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "clean_data = (transformed_df\n",
    " .select(\"date\", \"userid\", \"clicks\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .partitionBy(\"date\")\n",
    " .saveAsTable(\"landing.raw_clicks\")\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe3170c-913d-4b31-8718-e138ed6de4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read and check the optimization done to landing table\n",
    "\n",
    "Here we read and check how many partitions are created in Delta store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81a071b-de9c-4196-bfbe-acad1e321e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of Partitions :  22\n"
     ]
    }
   ],
   "source": [
    "landing_df = spark.read.format('delta').table('landing.raw_clicks')\n",
    "print('[INFO] Number of Partitions : ', landing_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43abdc4b-1b3b-4d78-888c-21b3b08e0eca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|  min_date|  max_date|\n+----------+----------+\n|2023-01-01|2024-12-31|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "landing_df \\\n",
    "    .select(min(\"date\").alias(\"min_date\"), max(\"date\").alias(\"max_date\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8013fa43-ebc5-4b82-b662-846820d31781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform Landing data for Conformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f5d188-3fa2-46f2-9071-0f57424cae15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "clean_landing_df = (landing_df\n",
    " .where(year(\"date\") == 2024)\n",
    " .dropDuplicates()\n",
    " .withColumn(\"month\", month(\"date\"))\n",
    " .select(\"date\", \"userid\", \"clicks\", \"month\")\n",
    " .orderBy(\"date\", \"userid\", \"clicks\")\n",
    ")\n",
    "\n",
    "clean_landing_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9788f95b-1cca-4526-9e17-02e87b03e22f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The data for 2024 is the cut-off year and the duplicates are dropped. Further an addition column for month is added which will be used to partition while stored in Delta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530af285-c252-4c22-a9ef-2be16b68579f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_landing_df \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"month\") \\\n",
    "    .saveAsTable(\"conformed.clicks_2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d7e36e-5b0e-41a9-b2c1-e16733a8e10b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating Views for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d12518-93af-46c9-9678-edd1cf86c085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE VIEW consumption.clicks_user1\n",
    "AS\n",
    "SELECT \n",
    "* FROM conformed.clicks_2024\n",
    "WHERE userid = 'user1';\n",
    "\n",
    "CREATE OR REPLACE VIEW consumption.clicks_user2\n",
    "AS \n",
    "SELECT\n",
    "* FROM conformed.clicks_2024\n",
    "WHERE userid = 'user2';\n",
    "\n",
    "CREATE OR REPLACE VIEW consumption.clicks_user100\n",
    "AS\n",
    "SELECT\n",
    "* FROM conformed.clicks_2024\n",
    "WHERE userid = 'user100 ';"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3564544708137322,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "User-Clicks-Analytics-Warehouse-Setup",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
